%!TEX root = ../dokumentation.tex

\chapter{Grundlagen}
	\section{Aussagenlogik}

Mit der Aussagenlogik können viele logische Zusammenhänge formal beschrieben werden. Eine Aussage ist eine Behauptung, die entweder wahr oder falsch sein kann, aber nicht beides. Beispiele für Aussagen sind: "`1+1=3"' und "`Das Gras ist grün."'. Solche Aussagen werden mit einzelnen Großbuchstaben $A, B, C, ...$ dargestellt und als Atome bezeichnet. Die Menge aller Atom-Symbole $\Sigma$ wird als Signatur bezeichnet. 

Atome können mithilfe von Junktoren verknüpft werden, um logische Zusammenhänge darzustellen. Das Ergebnis dieser Verknüpfung ist eine Formel. 
Es gibt fünf Junktoren: $\wedge$ (logisches Und, Konjunktion), $\vee$ (logisches Oder, Disjunktion), $\to$ (wenn, dann), $\leftrightarrow$ (genau dann, wenn) und $\neg$ (Negierung).

In der Logik werden außerdem die beiden Symbole $\top$ für wahr und $\bot$ für falsch eingesetzt. 

Folgende Tabelle zeigt, wie sich aus den Wahrheitswerten zweier Atome die Wahrheitswerte einer Formel ergeben.

\begin{table}[h]
	\centering
	\begin{tabular}{| c | c || c | c | c | c | c |}
		\hline
		&& Negierung & Konjunktion & Disjunktion & Implikation & Äquivalenz \\
		$A$ & $B$ & $\neg A$ & $A \wedge B$ & $A \vee B$ & $A \to B$ & $A \leftrightarrow B$ \\
		\hline
		$\top$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\top$ & $\top$ \\
		$\top$ & $\bot$ & $\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ \\
		$\bot$ & $\top$ & $\top$ & $\bot$ & $\top$ & $\top$ & $\bot$ \\
		$\bot$ & $\bot$ & $\top$ & $\bot$ & $\bot$ & $\top$ & $\top$ \\
		\hline
	\end{tabular}
	\caption{Übersicht der Wahrheitswerte von Junktoren}
	\label{table:Junktoren}
\end{table}

Formeln lassen sich rekursiv konstruieren und können dadurch beliebig weit verschachtelt werden. Die Konstruktionsregeln sind folgendermaßen definiert:
\begin{enumerate}
	\item Alle Symbole in $\Sigma$ sowie die Symbole $\top$ und $\bot$ sind Formeln.
	\item Wenn $F$ eine Formel ist, dann ist auch $\neg F$ eine Formel.
	\item Wenn $F$ und $G$ Formeln sind, dann sind auch $(F) \vee (G)$, $(F) \wedge (G)$, $(F) \to (G)$ und $(F) \leftrightarrow (G)$ Formeln.
\end{enumerate}
Wie bei mathematischen Operatoren z.B. Addition und Multiplikation, haben auch in der Logik die Operatoren eine Präzedenz. Sie beschreibt, wie stark die Operatoren binden, also in welcher Reihenfolge sie aufgelöst werden. Für diese Arbeit wird die in der Literatur am häufigsten eingesetzte Präzedenz verwendet:
$$\neg \quad \wedge \quad \vee \quad \to \quad \leftrightarrow$$
Die Negation bindet am stärksten und wird als erstes aufgelöst. Das Symbol für die Äquivalenz wird als letztes aufgelöst. Klammern können zusätzlich eingefügt werden, sie binden noch schwächer und werden als letztes aufgelöst.
Zwei Junktoren der gleichen Präzedenz werden von links nach rechts aufgelöst. Das heißt die Formel $A\to B\to C$ ist gleichbedeutend wie $((A\to B)\to C)$.

\subsection{weitere Begriffe}

Eine \textbf{Interpretation} I ist eine Abbildung, bei der jedem Atom einer der beiden Wahrheitswerte wahr oder falsch zugeordnet wird. 

$$I: \Sigma\to\{\top,\bot\}$$

Unter einer Interpretation $I$ ist dann auch jede Formel entweder wahr oder falsch. Die Auflösung einer Formel $F$ findet rekursiv statt, indem für alle Atome die Wahrheitswerte eingesetzt werden und die Teilformeln von $F$ von innen beginnend mit Tabelle \ref{table:Junktoren} aufgelöst werden.

Zwei Formeln werden \textbf{äquivalent} genannt, wenn sie für jede Interpretation den gleichen Wahrheitswert besitzen. Mit Äquivalenzumformungen kann aus einer Formel $F$ eine Formel $F'$ hergeleitet werden, die äquivalent zu $F$ ist. Die Äquivalenz wird mit dem Symbol $\equiv$ dargestellt. Ein Beispiel für eine Äquivalenzumformung ist die doppelte Negation. Wenn $F$ eine Formel ist, dann ist $\neg\neg F$ äquivalent zu $F$. Doppelte Negationen können in jeder beliebigen Position einer Formel eingefügt und entfernt werden. Äquivalenzen und Implikationen lassen sich durch Äquivalenzumformungen in Kon- und Disjunktionen umformen:
\begin{align*}
	A \to B &\equiv \neg A \vee B \\
	A \leftrightarrow B &\equiv (A \wedge B) \vee (\neg A \wedge \neg B)	
\end{align*}

Eine Formel, die unter keiner Interpretation wahr ist, heißt \textbf{unerfüllbar}. Eine Formel, die unter jeder Interpretation wahr ist, heißt \textbf{allgemeingültig}.

Ein \textbf{Literal} $L$ ist entweder ein Atom $A$ oder ein negiertes Atom $\neg A$. 
		\subsection{Ableitung}
Aus zwei Formeln kann durch logisches Schlussfolgern eine neue Formel hergeleitet werden. Dieser Prozess wird Ableitung genannt. Eine Ableitung bei der aus den Formeln $F_1$ und $F_2$ die Formel $F_3$ abgeleitet wird, wird folgendermaßen dargestellt:
$$\frac{F_1 \quad F_2}{F_3}$$
Semantisch bedeutet diese Ableitung, dass $F_3$ aus $F_1$ und $F_2$ folgt, also dass für jede Interpretation unter der $F_1 \wedge F_2$ wahr ist, auch $F_3$ wahr ist.
Das Beispiel 
$$\frac{A\quad A\to B}{B}$$
sagt aus: "`Immer wenn $A$ wahr ist und aus $A$ $B$ folgt, dann muss auch $B$ wahr sein."' Diese Ableitungsregel wird als Modus Ponens bezeichnet. \cite{Hoffmann2008TI}

\subsection{Normalformen}\label{section:Normalformen}
Es gibt mehrere Normalformen für die Aussagenlogik. Es lässt sich zeigen, dass jede beliebige Formel in die folgenden Normalformen transformiert werden kann.
\paragraph{Konjunktive Normalform (KNF)}
Eine Formel $F$, die in konjunktiver Normalform (KNF) vorliegt, besteht aus Konjunktionen einzelner Disjunktionen:
$$F=(L_{1,1} \vee L_{1,2} \vee \ldots \vee L_{1,n_1}) \wedge
   (L_{2,1} \vee \ldots \vee L_{2,n_2}) \wedge \ldots \wedge
   (L_{m,1} \vee \ldots \vee L_{m,n_m})$$
Die KNF hat den Vorteil, dass die Unerfüllbarkeit eines einzelnen Disjunktionsblocks zur Unerfüllbarkeit der gesamten Formel führt. Es ist dadurch einfach die Unerfüllbarkeit zu beweisen. Erfüllbarkeit und Allgemeingültigkeit sind vergleichsweise aufwändig zu beweisen.
\paragraph{Disjunktive Normalform}
Eine Formel $F$ in disjunktiver Normalform hat folgende Struktur:
$$F=(L_{1,1} \wedge L_{1,2} \wedge \ldots \wedge L_{1,n_1}) \vee
(L_{2,1} \wedge \ldots \wedge L_{2,n_2}) \vee \ldots \vee
(L_{m,1} \wedge \ldots \wedge L_{m,n_m})$$
Komplementär zur KNF können die Erfüllbarkeit und die Allgemeingültigkeit von Formeln sehr einfach nachgewiesen werden. Dazu muss nur eine einzlne Teilformel erfüllbar bzw. allgemeingültig sein. Unerfüllbarkeit ist schwierig nachzuweisen.

\paragraph{Klauselnormalform}
Die Klauselnormalform ist eine Alternative Schreibweise zur konjunktiven Normalform, die aber den gleichen Zusammenhang darstellt.
$$\{\{A_{1,1},A_{1,2},\ldots,A_{1,n_1}\},
\{A_{2,1},\ldots,A_{2,n_2}\},\ldots,
\{A_{m,1},\ldots,A_{m,n_m}\}\}$$

Syntaktisch beschreiben die beiden Normalformen unterschiedliche Dinge. Bei der konjunktiven Normalform handelt es sich um eine Konjunktion einzelner Disjunktionen, wohingegen die Klauselnormalform eine Menge von Klauseln, wobei jede Klausel eine Menge von Literalen ist.
Semantisch stellt beide Formen denselben Zusammenhang dar. Aus jeder Klausel muss mindestens ein Literal wahr sein, damit die gesamte Klauselmenge wahr ist.
Gegenüber der KNF wird die Klauselnormalform oft bevorzugt, da sie eine kürzere und übersichtlichere Syntax hat. In der Klauselnormalform kann auch eine leere Klausel $\{\}$ oder $\emptyset$ vorkommen. Die leere Klausel wird immer zu falsch ausgewertet und macht somit die gesamte Klauselmenge unerfüllbar. Unerfüllbarkeit kann somit bewiesen werden, indem die leere Klausel abgeleitet wird.

	\section{Prädikatenlogik erster Stufe}
		\subsection{Begriffe}

Die Prädikatenlogik erster Stufe ist eine Erweiterung zur Aussagenlogik. Um die drei Aussagen "`Albert ist ein Mensch"', "`Bernhard ist ein Mensch"' und "`Cäsar ist ein Mensch"' in der Aussagenlogik dazustellen, müsste man drei Atome $A, B$ und $C$ einführen. Anstatt Aussagen, gibt es in der Prädikatenlogik \textbf{Prädikate}. Um das genannte Beispiel in der Prädikatenlogik zu formalisieren, könnte man ein Prädikat $P(x)$ einführen, das den Satz "`x ist ein Mensch"' zum Ausdruck bringt. Für die \textbf{Variable} $x$ können dann Werte eingesetzt werden, zum Beispiel $a, b$ und $c$ für Albert, Bernhard und Cäsar. $a, b$ und $c$ bezeichnet man als Konstanten.
Prädikate können eine beliebige Stelligkeit besitzen, das zweistellige Prädikat $Q(x,y)$ könnte zum Beispiel den Zusammenhang "`x ist der Bruder von y"' darstellen.

Zusätzlich gibt es in der Prädikatenlogik \textbf{Funktionen}. Wie Prädikate können Funktionen beliebig viele Stellen haben. Eine einstellige Funktion bildet Konstanten auf Konstanten ab. Eine n-stellige Funktion bildet n-Tupel von Konstanten auf Konstanten ab.

\textbf{Terme} sind Zusammensetzungen aus Variablen, Konstanten und Funktionen und können rekursiv konstruiert werden. Dabei ist jede einzelne Variable und jede einzelne Konstante ein Term und jede Funktion $f(t_1,\ldots,t_n)$ ebenso, wobei $t_1,\ldots,t_n$ auch Terme sein müssen.

Aus einem $n$-stelligen Prädikatensymbol und $n$ Termen lässt sich ein \textbf{Atom} erstellen. Beispiele für Atome sind:
$$P(x) \quad P(a) \quad P(f(a)) \quad Q(x, g(a,y)) \quad Q(g(a,f(b)), f(f(a)))$$
Wie in der Aussagenlogik lassen sich Atome mit Junktoren zu \textbf{Formeln} verknüpfen. Zusätzlich zu den 5 Junktoren gibt es noch zwei \textbf{Quantoren} $\forall$ und $\exists$. Für Quantoren gibt es eine zusätzliche Konstruktionsregel:
Wenn $F$ eine Formel und $x$ eine Variable ist, dann sind $\forall x(F)$ und $\exists x(F)$ ebenso Formeln.
Semantisch bedeutet $\forall x(F)$: "`Für jedes $x$ gilt die Formel $F$"'.  $\exists x(F)$ heißt: "`Es existiert mindestens ein $x$, für das die Formel $F$ gilt."'

Tabelle \ref{table:symbole} zeigt eine Auflistung von Begriffen und deren Symbole, die im Folgenden eingesetzt werden.

\begin{table}
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Begriff & Symbole \\ \hline\hline
		Konstante & $a,b,c$ \\\hline
		Variable & $x,y,z$ \\ \hline
		Funktion & $f, g, h$ \\\hline
		Prädikat & $P, Q, R$ \\\hline
	\end{tabular}
	\begin{tabular}{|c|c|}
		\hline
		Begriff & Symbole \\ \hline\hline
		Atom & $A, B$ \\\hline
		Literal & $L, K$ \\\hline
		Formel & $F, G, H$ \\\hline
		Klausel & $C, D, E$ \\\hline
	\end{tabular}
	\caption{Symbole in der Prädikatenlogik}
	\label{table:symbole}
\end{table}

\subsection{Normalformen}
\paragraph{pränexe Normalform} Prädikatenlogische Formeln können durch die Vermischung von Junktoren und Quantoren schnell komplex werden. Die pränexe Normalform sorgt für eine klare Trennung von Junktoren und Quantoren. Die allgemeine Form ist gegeben durch
$$Q_1x_1\ldots Q_nx_n(F),$$
wobei jedes $Q$ entweder $\forall$ oder $\exists$ ist und $F$ eine quantorenfreie Formel ist. \cite{Chang1973Symb}
\paragraph{Skolemnormalform} Die Quantoren einer Formel $F_0$ können durch Äquivalenzumformungen nicht vollständig entfernt werden, aber es kann eine erfüllbarkeitsgleiche Formel $F_1$ aufgestellt werden, die keine Quantoren enthält. Die Skolemisierung ist ein Prozess, der aus $F_0$ in pränexer Normalform $F_1$ erstellt. $F_1$ liegt dann  in Skolemnormalform vor. Die erfüllbarkeitsgleichen Formeln $F_0$ und $F_1$ müssen nicht zwangsweise äquivalent sein, aber wenn $F_0$ (un-)erfüllbar ist, dann gilt das gleiche für $F_1$. Für viele Anwendungen ist nur die (Un-)erfüllbarkeit von Interesse und die konkreten Interpretationen sind irrelevant. Die Skolemisierung ist somit sehr nützlich, da Prüfung auf (Un-)erfüllbarkeit auch auf die vereinfachte Formel $F_1$ angewendet werden kann.
\paragraph{Klauselnormalform}
Die Skolemnormalform enthält keine Quantoren mehr, deshalb kann aus ihr nach den Regeln der Aussagenlogik die Klauselnormalform aufgestellt werden.

	\section{Resolutionsverfahren}

Theorembeweiser haben das Ziel, aus einer gegebenen Wissensbasis eine Aussage zu beweisen. Das Wissen liege in Klauselnormalform $N=\{C_1, C_2, ..., C_n \}$ vor, die Vermutung sei eine weitere Klausel $D$. Um zu beweisen, dass $D$ aus $N$ folgt gibt es zwei Ansätze: 
\begin{itemize}
\item Man weißt nach, dass aus der Klauselmenge $N$ die Klausel $D$ ableitbar ist. 
\item Man weißt nach, dass die Klauselmenge $N\cup\{\neg D\}$  unerfüllbar ist, das heißt, dass die leere Klausel $\{\}$ ableitbar ist. Das Zeichen $\neg D$ entspricht der Negierung von $D$.
\end{itemize}

Beim Resolutionsverfahren wird der zweite Ansatz gewählt, das heißt die negierte Form des zu beweisenden Ziels wird zur Klauselmenge hinzugefügt. Danach werden mittels Resolution jeweils zweier gegebenen Klauseln neue Klauseln abgeleitet. Die neuen Klauseln werden ebenfalls zur Klauselmenge hinzugefügt. Sobald die leere Klausel abgeleitet wurde, terminiert das Programm und die Vermutung wurde bewiesen. Das Resolutionsverfahren ist vollständig fürs Widerlegen, das heißt aus jeder widersprüchlichen Klauselmenge lässt sich die leere Klausel ableiten. Der Beweisprozess kann beliebig lange dauern, das heißt solange kein Beweis gefunden wurde, kann man nicht sicher sein, ob die Klauselmenge erfüllbar oder unerfüllbar ist. \cite{Chang1973Symb}

		\subsection{Resolution in der Aussagenlogik}
Gegeben sei eine Klauselmenge $S$ mit zwei Klauseln $C$ und $D$, die sich aus den Literalen $L_i$ und $K_i$ zusammensetzen. Jedes Literal ist entweder ein Atom $A$, oder ein negiertes Atom $\neg A$.
$$S=\{C,D\}, \quad C=\{L_1, L_2, ..., L_n\}\quad, D=\{K_1, K_2, ..., K_m\}$$
Wenn das Literal $L_1$ aus Klausel $C$ in negierter Form in $D$ vorkommt, also $L_1\equiv\neg K_1$, dann kann aus $C$ und $D$ die Resolvente $E$ gebildet werden mit:
$$\frac{C\quad D}{E}L_1=\frac{C \quad D}{(C \cup D) \backslash \{L_1, K_1\}}L_1
= \frac{\{L_1, L_2, ..., L_n\}\quad\{K_1, K_2, ..., K_m\}}{\{L_2, ..., L_n, K_2, ..., K_m\}}L_1$$

Anschaulich kann die Resolution folgendermaßen erklärt werden: 
Wenn das Literal $L_1$ unter einer Interpretation wahr ist, dann wird die gesamte Klausel $C$ wahr. Sollte $L_1$ falsch sein, dann muss mindestens eines der übrigen Literale in C wahr sein.
Das gleiche gilt für $K_1$ in der Klausel $D$.
Da $L_1$ und $K_1$ komplementär sind, muss unter jeder Interpretation eines der beiden Literalen $L_1$ und $K_1$ falsch sein. Damit beide Klauseln zugleich wahr werden, muss also in einer der beiden Klauseln mindestens eines der übrigen Literale wahr sein. Die Resolvente $E$ bringt diesen Zusammenhang zum Ausdruck, indem sie die Literale aus $C$ und $D$ ohne $L_1$ und $K_1$ vereinigt.

Das Anwachsen der hergeleiteten Klauseln kann mit dem Res-Operators dargestellt werden. Der Res-Operator angewendet auf eine Klauselmenge ergibt die Klauselmenge vereinigt mit der Resolvente eines einzelnen Resolutionsschritts.
$$Res(S)=S \cup \{E\}$$
$Res^*(S)$ enthält alle Klauseln in $S$ sind und alle Resolventen, die aus $Res^*(S)$ gebildet werden können. Diese rekursiv definierte Klauselmenge enthält also alle Klauseln, die mit beliebig vielen Resolutionsschritten aus $S$ abgeleitet werden können. Ist die leere Klausel in $Res^*(S)$ enthalten, so ist die Vermutung bestätigt.
		\subsection{Resolutionsverfahren in der Prädikatenlogik}
Äquivalent zur Aussagenlogik kann auch in der Prädikatenlogik die Resolution angewendet werden. Im Gegensatz zur Aussagenlogik ist die Anzahl der Literale nicht begrenzt, sondern kann durch die Kombination von Variablen, Konstanten und Funktionen beliebig erweitert werden. Meist gibt es unendlich viele potenzielle Literale. Dadurch verringert sich die Wahrscheinlichkeit, dass neue Klauseln abgeleitet werden können. Damit das Resolutionsverfahren für die Prädikatenlogik weiterhin vollständig bleibt, wird die Resolutionsregel erweitert, außerdem wird die Faktorisierung als zweite Ableitungsregel eingeführt. \cite{Chang1973Symb}
\paragraph{Resolution}
Die beiden Klauseln $C=\{P(x), Q(x)\}, D=\{\neg P(f(a)), R(y)\}$ lassen sich mit der bisherigen Resolutionsregel nicht resolvieren, da keine komplementären Literale in den Klauseln vorkommen.
Mit einer Substitution $\sigma$ können die Klauseln so angepasst werden, dass sich die Resolution anwenden lässt. $\sigma$ beschreibt, wie Variablen ersetzt werden. Die entstandenen Klauseln aus $C$ und $D$ werden $C\sigma$ und $D\sigma$ genannt.
$$\sigma=\{x \mapsto f(a)\}$$
$$C\sigma=\{P(f(a)), Q(f(a))\} \quad\quad D\sigma=\{\neg P(f(a)), R(x)\} $$

Anschließend lässt sich die Resolvente bilden. Formal schreibt man

$$\frac{C \quad D}{E}\sigma = \frac{C\sigma \quad D\sigma}{E}= \frac{\{P(f(a)), Q(f(a))\} \quad \{\neg P(f(a)), R(x)\}}{\{Q(f(a)), R(x)\}}$$

Die allgemeine Form ist damit
$$\frac{C \cup \{L\} \quad D \cup \{\neg M\}}{(C \cup D)\sigma} \sigma \quad\text{mit}\quad \sigma=\text{mgu}(L, M)$$
Die Funktion $\text{mgu}(L, M)$ steht für "`most general unifier"' und ist die allgemeinste Substitution, die die Literale $L$ und $M$ gleich werden lässt \cite{Haifani2021Sos}.
Es muss zu jeder Zeit garantiert werden, dass die beiden Klauseln keine gemeinsamen Variablen besitzen. Diese Anforderung kann immer erfüllt werden, indem die Variablen einer Klausel beliebig umbenannt werden \cite{Hoffmann2008TI}.
\paragraph{Faktorisierung}
Die Faktorisierung ist eine Ableitungsregel, mit der aus einer einzelnen Klausel neue Klauseln abgeleitet werden können. Ziel ist, eine Substitution zu finden, die zwei Literale $L$ und $M$ innerhalb der Klausel gleich werden lässt, damit sich die Größe der Klausel verringert. \cite{Chang1973Symb}
Die allgemeine Form der Ableitungsregel lautet:
$$\frac{C \cup \{L,M\}}{(C \cup \{L\})\sigma}\sigma \quad\text{mit}\quad\sigma=\text{mgu}(L,K)$$
	
	\section{Set-Of-Support-Strategien}
Ein Problem beim Resolutionsverfahren ist, dass aus einer endlichen Menge von Axiomen häufig unendlich viele neue Klauseln abgeleitet werden können. Dadurch gibt es keine Garantie, dass in endlicher Zeit ein Beweis gefunden wird. Selbst wenn ein Beweiser bereits einen Widerspruch herleiten konnten, bedeutet das nicht, dass jeder andere Beweiser auch einen Widerspruch finden wird.

Um die Chance, einen Beweis zu finden, zu vergrößern, wurde eine Vielzahl von Strategien entwickelt, die bei der Durchführung der Resolution angewendet werden kann. Beim gewöhnlichen Resolutionsverfahren gibt es keine vorgeschriebene Reihenfolge, in der die Klauseln abgearbeitet werden. Meist iteriert der Algorithmus der Reihe nach oder willkürlich über die Klauseln. Die Grundidee vieler Strategien ist dass die Klauseln vom Beweiser nicht willkürlich ausgewählt werden. Stattdessen sollen Algorithmen beurteilen, welche möglichen Schritte vielversprechend sind, um unnötige Schritte zu vermeiden. Zwei sehr intuitive Strategien sind zum Beispiel:
\begin{itemize}
	\item Heuristiken: Aus Klauseln mit wenig Literalen ist die Wahrscheinlichkeit größer, einen Widerspruch zu finden, als aus Klauseln mit sehr vielen Literalen. Kleinere Klauseln werden deshalb priorisiert verarbeitet.
	\item Tautologien löschen: Tautologien, also Klauseln, die unter jeder Interpretation wahr sind, tragen nichts zur Beweisführung bei. Sie können deshalb für das gesamte Beweisverfahren ignoriert werden.
\end{itemize}

Eine Reihe komplexerer Strategien sind die Set-of-Support-Strategien. Sie wurden erstmals 1965 von Lawrence Wos et al. vorgestellt. \cite{Wos1965Sos}

		\subsection{Prinzip}
Bei den Set-Of-Support-Strategien (SOS-Strategien) wird die unerfüllbare Menge $M$ aller Klauseln in zwei disjunkte Teilmengen $N$ und $S$ unterteilt. Die Menge $N$ ist erfüllbar. Aus $N$ alleine kann also kein Widerspruch hergeleitet werden. Erst beim Hinzunehmen vom sogenannten Set-Of-Support $S$ kann die leere Klausel abgeleitet werden. Nach der Einteilung wird wie gewöhnlich das Resolutionsverfahren durchgeführt. Die neuen Klauseln, die durch Resolution oder Faktorisierung gebildet werden, werden zu $S$ hinzugefügt. 

Die einzige Einschränkung bei den Set-Of-Support-Strategien ist, dass bei der Ableitung neuer Klauseln mindestens eine der Eltern-Klausel im Set-Of-Support liegen muss. Dies hat den Vorteil, dass weniger Klauseln miteinander kombiniert werden können und es weniger mögliche Resolutionsschritte gibt. Somit soll die Chance vergrößert werden, einen Widerspruch zu finden.

Bei einer gegebenen Klauselmenge gibt es sehr viel Möglichkeiten, die Klauseln in zwei Mengen $N$ und $S$ einzuteilen. Da die Laufzeit der Beweissuche sehr wichtig ist, ist es nicht sinnvoll sehr komplexe Einteilungen vorzunehmen, da sonst der gesamte Performance-Vorteil durch den Mehraufwand der Einteilung wieder verloren geht. Stattdessen liegt der Fokus auf Einteilungen, die sehr einfach vorzunehmen sind. Im Folgenden werden fünf Einteilungen vorgestellt:
\begin{enumerate}
	\item Alle Axiome werden zu $N$ hinzugefügt und die Klauseln der negierten Vermutung werden zu $S$ hinzugefügt. Die Bedingung, dass $N$ erfüllbar ist, ist in jedem Fall gegeben, da von Axiomen angenommen wird, dass sie immer erfüllbar sind.
	\item Alle Klauseln, die nur negative Literale enthalten, werden zu $S$ hinzugefügt, die restlichen zu $N$. $N$ ist erfüllbar, da jede Klausel in $N$ mindestens ein positives Literal enthält. Somit macht eine Interpretation, die alle Literale wahr werden lässt, jede Klausel wahr.
	\item Alle Klauseln, die nur positive Literale enthalten, werden zu $S$ hinzugefügt, die restlichen $N$. $N$ ist erfüllbar, da die Interpretation "`alle Literale falsch"' die Klauselmenge wahr macht.
	\item Alle Klauseln, die nur positive Literale enthalten, werden zu $N$ hinzugefügt, die restlichen zu $S$. Ähnlich wie bei 2. ist $N$ erfüllbar, da die Interpretation unter der alle Literale wahr werden, ein Modell der Klauselmenge ist.
	\item Alle Klauseln, die nur negative Literale enthalten, werden zu $N$ hinzugefügt. Äquivalent zu 3. ist diese Klauselmenge erfüllbar, unter der Interpretation, dass alle Literale für beliebige Variablenersetzungen falsch sind.
\end{enumerate}

	
\subsection{Vorteil gegenüber gewöhnlicher Resolution}
Der Einsatz von SOS-Strategien ist nur dann sinnvoll, wenn zwei Bedingungen erfüllt sind:
\begin{itemize}
	\item Das Resolutionsverfahren muss mit einer SOS-Strategie weiterhin vollständig fürs Widerlegen sein. Andernfalls könnte es passieren, dass der Beweiser eine unerfüllbare Klauselmenge für erfüllbar hält, da kein Widerspruch hergeleitet werden kann.
	\item Die Laufzeit der Beweisführung sollte mit einer SOS-Strategie im Mittel signifikant schneller sein, als ohne.

\end{itemize}
Die Vollständigkeit wird im folgenden Abschnitt erläutert. Das Laufzeitverhalten wird in Kapitel 5 analysiert.

Auch ohne Laufzeitanalyse lässt sich zeigen, dass die Methoden 4. und 5. keinen Vorteil in der Beweissuche liefern. Dies lässt sich folgendermaßen erklären. Die Resolution zweier Klauseln kann dann und genau dann durchgeführt werden, wenn in den Klauseln Literale vorkommen, die durch Substitution komplementär aber sonst identisch gemacht werden können. In SOS-Strategie 4 und 5 kommen in allen Klauseln der Basisklauselmenge entweder nur positive Literale oder nur negative Literale vor. Auch ohne SOS-Strategie kann deshalb auf zwei dieser Klauseln keine Resolution angewendet werden. Das heißt, dass ein Verbot dieser Resolutionsschritte keine Auswirkung auf den Ablauf der Beweisführung hat und somit auch keinen Zeitvorteil liefern kann. Diese beiden Strategien werden in dieser Arbeit deshalb nicht weiter betrachtet.

		\subsection{Vollständigkeit}		
Die Resolution ist mit Set-Of-Support-Strategien weiterhin vollständig für das Widerlegen. Dies wurde bereits 1965 von Lawrence Wos et al. gezeigt. In dieser Arbeit wird nicht der gesamte Beweis für die Prädikatenlogik erster Stufe gezeigt, sondern lediglich ein Beweis für die Vollständigkeit in der Aussagenlogik. Dieser Beweis orientiert sich an der Veröffentlichung von Wos et al. \cite{Wos1965Sos}

\paragraph{Theorem} Werden aus einer unerfüllbaren variablenlosen Klauselmenge $M$ zwei neue Klauselmengen $N$ und $S$ erstellt, sodass gilt: 
$$N \cup S = M \quad\quad N \cap S = \emptyset \quad\quad S \neq \emptyset \quad\quad N \text{ ist erfüllbar,}$$
so kann aus $M$ mit $S$ als Set of Support die leere Klausel abgeleitet werden.

\paragraph{Beweis durch vollständige Induktion}
Besteht die Signatur $\Sigma$ aus einem einzigen Symbol $A_1$, dann können nur die beiden Literale $A_1$ und $\neg A_1$ in den Klauseln von $M$ vorkommen.
Es ergeben sich vier mögliche Klauseln für $M$: 
\begin{enumerate}
	\item $\{A_1\}$
	\item $\{\neg A_1\}$
	\item Die leere Klausel $\{\}$
	\item Die Tautologie $\{A_1,\neg A_1\}$
\end{enumerate}
Wenn die Tautologie in $M$ enthalten ist, trägt sie zum Beweis nichts bei, deshalb wird sie zur Übersichtlichkeit weggelassen.
Für den Fall, dass die leere Klausel in $M$ vorkommt, muss sie im Set-Of-Support liegen, da $N$ erfüllbar sein muss. Da die leere Klausel bereits vorliegt, muss kein Beweis gesucht werden, das Theorem hält für diesen Fall also stand. 

Falls die leere Klausel nicht in $M$ liegt, müssen sowohl die Klauseln 1 und 2 in $M$ enthalten sein, da $M$ unerfüllbar sein muss. Es gibt drei verschiedene Unterteilungen, wie $M$ in $N$ und $S$ unterteilt werden können (siehe Tabelle \ref{table:sos-one-atom})
\begin{table}[h]
	\centering
	\begin{tabular}{c|c}
		N & S \\ \hline
		$\big\{\{A_1\}\big\}$ & $\big\{\{\neg A_1\}\big\}$ \\
		$\big\{\{\neg A_1\}\big\}$ & $\big\{\{A_1\}\big\}$ \\	
		$\big\{\big\}$ & $\big\{\{A_1\},\{\neg A_1\}\big\}$
	\end{tabular}
	\caption{Drei mögliche SOS-Einteilungen einer unerfüllbaren Klauselmenge mit einem Atom}
	\label{table:sos-one-atom}
\end{table}

Für alle drei Unterteilungen ist es möglich mit Set-Of-Support-Regeln einen Widerspruch herzuleiten, indem die Resolvente beider Klauseln gebildet wird. Das Theorem ist dementsprechend für alle Klauseln mit nur einem Atom gültig.

\paragraph{Induktionsschritt} Im Folgenden wird angenommen, dass das Theorem für eine Anzahl von $i-1$ Atomen in $\Sigma$ gültig ist. Unter dieser Annahme wird bewiesen, dass dass das Theorem für $i$ Atome in $\Sigma$ ebenfalls gültig ist.

Die Signatur sei nun folgendermaßen definiert:
$\Sigma=\{A_1, A_2, ... , A_{i-1}, A_i\}$
Die Klauselmenge $M$ sei bereits in eine erfüllbare Menge $N$ und ein Set-Of-Support $S$ unterteilt worden.
Als nächstes wird eine Fallunterscheidung vorgenommen. Zuerst wird $A_i$ als wahr interpretiert. Damit können alle Klauseln, die $A_i$ enthalten, gestrichen werden. Aus den restlichen Klauseln, wird das Literal $\neg A_i$ entfernt. Diese beiden Operationen werden auf $M$, $N$ und $S$ angewendet. Die daraus entstehenden Klauseln werden mit $M_1$, $N_1$ und $S_1$ bezeichnet.
Danach wird $A_i$ als falsch interpretiert. Alle Klauseln, die $\neg A_i$ enthalten, können entfernt werden. Aus den übrigen Klauseln, wird das Literal $A_i$ entfernt. Die neu entstandenen Klauselmengen werden als $M_2$, $N_2$, $S_2$ bezeichnet.
Jede Klausel $C$ in $M_1$ und $M_2$ besitzt somit eine korrespondierende Klausel $C'$, die in $M$ enthalten ist. $C'$ enthält mindestens die Literale von $C$, kann aber zusätzlich eins der Literale $A_i$ oder $\neg A_i$ enthalten. 

Da $M$ unerfüllbar ist, müssen sowohl $M_1$ als auch $M_2$ unerfüllbar sein. Dies lässt sich damit begründen, dass $M$ für jede beliebige Interpretation mit falsch ausgewertet wird und somit auch alle Interpretationen in den beiden Fälle $A_i \equiv \top$ und $A_i \equiv \bot$ die Klauselmenge falsch machen. Da $N$ erfüllbar ist, muss entweder $N_1$ oder $N_2$ oder beide erfüllbar sein.

Wenn $N_1$ erfüllbar ist, dann kann aus $M_1$ mit $S_1$ als Set-Of-Support ein Widerspruch abgeleitet werden. Diese Folgerung ergibt sich daraus, dass $M_1$ maximal $i-1$ verschiedene Atome enthalten kann und für die Induktion angenommen wird, dass das Theorem für $i-1$ Atome erfüllt ist. Gleiches gilt für $M_2$, wenn $N_2$ erfüllbar ist. 

Es wird zuerst der Fall berücksichtigt, dass sowohl $N_1$ als auch $N_2$ erfüllbar sind. Die Deduktionen, mit denen aus $M_1$ bzw. $M_2$ die leere Klausel abgeleitet werden kann, werden $D_1$ bzw. $D_2$ benannt. Das Beispiel in Tabelle \ref{table:Sos-both-unsatisfiable} veranschaulicht den Zusammenhang.
\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline 
		& Basisklauselmenge & Set-Of-Support & Deduktion\\ \hline\hline
		$M$ &
		$N=\big\{\{A_1\},\{\neg A_2,\neg A_3\}\big\}$ &
		$S=\big\{\{A_2\},\{\neg A_1,A_3\}\big\}$ & 
		$D$ ist gesucht \\ \hline
		
		$M_1$ &
		$N_1=\big\{\{A_1\},\{\neg A_2\}\big\}$ &
		$S_1=\big\{\{A_2\}\big\}$ &
		$D_1=\big(\{A_2\},\{\neg A_2\},\{\}\big)$ \\ \hline
		
		$M_2$ &
		$N_2=\big\{\{A_1\}\big\}$ &
		$S_2=\big\{\{A_2\},\{\neg A_1\}\big\}$ & 
		$D_2=\big(\{\neg A_1\},\{A_1\},\{\}\big)$ \\ \hline
	\end{tabular}
	\caption{Beispiel für eine Klauselmenge $M$, bei der $M_1$ und $M_2$ unerfüllbar sind.}
	\label{table:Sos-both-unsatisfiable}
\end{table}

 Werden die Resolutionsschritten von $D_1$ nicht auf die Klauseln in $M_1$ angewendet, sondern auf die korrespondierenden Klauseln in $M$, so ergibt sich eine neue Deduktion $D_1'$.
 Diese endet im Beispiel nicht mit der leeren Klausel, sondern mit der Klausel $\neg A_3$.
 $$D_1'=\big(\{A_2\},\{\neg A_2, \neg A_3\},\{\neg A_3\}\big)$$
 Genauso können die Schritte von $D_2$ auf $M$ angewendet werden. Damit ergibt sich $D_2'$ wie folgt:
 $$D_2'=\big(\{\neg A_1, A_3\},\{A_1\},\{A_3\}\big)$$
 Werden $D_1'$ und $D_2'$ zusammengeführt, kann als neuen Resolutionsschritt die leere Klausel abgeleitet werden. Die neue Deduktion wird als $D$ bezeichnet und ist ein Beweis für die Unerfüllbarkeit von $M$ mit $S$ als Set-Of-Support. Für das Beispiel ergibt sich $D$ mit
 $$D=D_1'\oplus D_2'\oplus\big(\{\}\big)=\big(\{A_2\},\{\neg A_2, \neg A_3\},\{\neg A_3\},\{\neg A_1, A_3\},\{A_1\},\{A_3\},\{\}\big) \text{ ,}$$
 wobei der Operator $\oplus$ als Aneinanderreihung von Tupeln verstanden wird.
 
 Anmerkung: Die Deduktionen $D_1'$ und $D_2'$ müssen im Allgemeinen nicht mit den Klauseln $\{\neg A_i\}$ und $\{A_i\}$ enden. Stattdessen können sie auch mit der leeren Klausel enden. Für diesen Fall müssen $D_1'$ und $D_2'$ nicht zusammengeführt werden, da die leere Klausel bereits gefunden wurde. Das Theorem  hält also dennoch stand.
 
 
 Wenn eine der beiden Klauselmengen $N_1$ und $N_2$ unerfüllbar ist, kann der Beweis nicht mit der gezeigten Methode geführt werden. Grund dafür ist, dass eine der beiden Deduktionen nicht die Set-Of-Support-Regeln befolgt, da $N_1$ bzw. $N_2$ nicht erfüllbar ist. Somit würde auch die zusammengeführte Deduktion $D$ nicht zwangsweise den Set-Of-Support-Regeln folgen.  Das Beispiel in Tabelle \ref{table:sos-one-satisfiable} mit $i=3$ veranschaulicht die Situation.
 
  \begin{table}[h]
 	\centering
 	\begin{tabular}{|l|l|l|l|}
 		\hline
 		& Basisklauselmenge & Set-Of-Support & Deduktion \\ \hline\hline
 		
 		$M$ &
 		$N=\big\{\{A_1\},\{\neg A_1, A_3\}\big\}$ &
 		\cellbreak{l}{
 			$S=\big\{\{\neg A_1, A_2\},$\\
 			$\quad\quad\{\neg A_1,\neg A_3\}\big\}$} &
 		$D$ wird gesucht\\ \hline
 		
 		$M_1$ &
 		$N_1=\big\{\{A_1\}\big\}$ &
 		\cellbreak{l}{
 			$S_1=\big\{\{\neg A_1, A_2\},$\\
 			$\quad\quad\{\neg A_1\}\big\}$} &
 		$D_1=\big(\{\neg A_1\},\{A_1\},\{\}\big)$\\ \hline
 		
 		$M_2$ &
 		$N_2=\big\{\{A_1\},\{\neg A_1\}\big\}$ &
 		$S_2=\big\{\{\neg A_1, A_2\}\big\}$ &
 		$D_2$ ist undefiniert \\ \hline
 	\end{tabular}
 \caption{Beispiel für eine Klauselmenge $M$, bei der $M_1$ unerfüllbar und $M_2$ erfüllbar ist.}
 \label{table:sos-one-satisfiable}
 \end{table}

Zuerst wird angenommen $N_1$ ist erfüllbar und $N_2$ unerfüllbar. Für $M_1$ gibt es eine Deduktion $D_1$, die die die Set-Of-Support-Regeln befolgt und mit der leeren Klausel endet. Äquivalent zum vorherigen Beispiel gibt es somit auch eine Deduktion $D_1'$ für $M$, die mit der Klausel $\{\neg A_i\}$ endet. Für das Beispiel gilt:
 $$D_1'=\big(\{\neg A_1, \neg A_3\},\{A_1\}, \{\neg A_3\}\big) \text{ .}$$
 
 $D_2$ ist undefiniert, da durch die Unerfüllbarkeit von $N_2$ die Set-Of-Support-Eigenschaften nicht erfüllt sind. Die Klauselmenge $N_2$ setzt sich aus zwei Arten von Klauseln zusammen:
 \begin{enumerate}
 	\item Klauseln, die in $N$ vorkommen. Die Menge dieser Klauseln wird mit $T$ bezeichnet. $T$ ist erfüllbar, da $N$ erfüllbar ist und $T$ eine Teilmenge von $N$ ist.
 	\item Klauseln, die nicht in $N$ vorkommen, da sie durch das Entfernen von $A_i$ modifiziert wurden. Die Menge dieser Klauseln wird mit $U$ bezeichnet.
 \end{enumerate}
 Somit kann die unerfüllbare Menge $N_2$ in eine erfüllbare Teilmenge $T$ und ein Set-Of-Support $U$ geteilt werden. $N_2$ enthält maximal $i-1$ verschiedene Atome. Für den Induktionsbeweis wird vorausgesetzt, dass das Theorem für $i-1$ erfüllt ist, weshalb aus $N_2$ mit Set-Of-Support-Regeln die leere Klausel abgeleitet werden kann. Diese Deduktion wird mit $D_3$ bezeichnet. Für das Beispiel gilt:
 $$N_2=\big\{\{A_1\},\{\neg A_1\}\big\} \quad\quad T=\big\{\{A_1\}\big\} \quad\quad U=\big\{\{\neg A_1\}\big\} \quad\quad D_3=\big(\{\neg A_1\},\{A_1\},\{\}\big)$$
 
 Jede der Klauseln in $U$ ist aus einer Klausel von $N$ durch Entfernung des Atoms $A_i$ entstanden. Anders ausgedrückt kann jede Klausel in $U$ abgeleitet werden, indem die korrespondiere Klausel in $N$ mit der Unit-Klausel $\{\neg A_i\}$ resolviert wird. Die Unit-Klausel $\{\neg A_i\}$ kann wie bereits gezeigt aus $N$ und $S$ mit der Deduktion $D_1'$ abgeleitet werden, somit können aus $N$ und $S$ auch alle Klauseln, die in $U$ vorkommen, abgeleitet werden. Daraus wiederum kann die leere Menge hergeleitet werden.
 
 Formal ergibt sich die Deduktion $D$ aus der Aneinanderreihung von (1) $D_1'$, (2) allen Klauseln in $N$, die durch Entfernung von $\neg A_i$ in eine Klausel von $U$ umgewandelt werden und (3) $D_3$. Für das Beispiel gilt:
 \begin{align*}
 	D &=D_1'\oplus\big(\{\neg A_1,A_3\}\big)\oplus D_3 \\
 	&= \big(\{\neg A_1, \neg A_3\}, \{A_1\}, \{\neg A_3\}, \{\neg A_1,A_3\}, \{\neg A_1\},\{A_1\},\{\} \big)
 \end{align*}

Sollte $N_1$ unerfüllbar sein, aber dafür $N_2$ erfüllbar, dann muss für den Beweis lediglich $A_i$ mit $\neg A_i$ vertauscht werden. Somit ist der Beweis für alle Fälle abgedeckt, das Theorem ist also für beliebiges $i \geq 0$ gültig.

Da die Aussagenlogik ein Sonderfall der Prädikatenlogik ist, gilt das Theorem für eine Teilmenge aller prädikatenlogischen Formeln. Mit weiteren Lemmas kann die Vollständigkeit auf alle Formeln der Prädikatenlogik erster Stufe erweitert werden. \cite{Wos1965Sos}
		
	\section{TPTP und StarExec}
	
TPTP steht für "`Thousands of Problems for Theorem Provers"' und ist eine Sammlung prädikatenlogischer Probleme \cite{Sutcliff2021TPTP}. Jedes Problem setzt sich aus einer Menge von Formeln und/oder Klauseln zusammen und ist entweder erfüllbar oder unerfüllbar. Für Theorembeweiser können diese Probleme als Benchmark eingesetzt werden, um sie auf Fehler zu prüfen und um ihre Performanz zu testen. Die aktuelle Version 7.5.0 beinhaltet 24098 Probleme.
Um die Performanz der Beweissuche mit anderen Beweisern vergleichen zu können, sollte Hardware benutzt werden, die eine konsistente Rechenleistung bereitstellt. StarExec ist ein Service, der genau dies ermöglicht. Es handelt sich dabei um mehrere Rechner, die zu einem Cluster vereint sind. \cite{Sutcliff2022StarExec}
 Rechenaufgaben, bei denen Beweiser beispielsweise TPTP Probleme lösen, werden als Jobs bezeichnet und können über eine Webseite konfiguriert werden. Ein Job setzt sich aus einer Menge von Beweisern und einer Menge von Problemen zusammen. Das Kreuzprodukt beider Mengen entspricht allen Paarungen aus einem Beweiser und einem Problem. Für jede Paarung führt der Beweiser eine Beweissuche für das Problem durch. Die möglichen Resultate lassen sich in vier Gruppen einteilen. 
\begin{enumerate}
	\item Unerfüllbar: Die leere Menge wurde gefunden, es liegt ein Beweis vor.
	\item Erfüllbar: Das Problem wurde so weit saturiert, dass ein Modell gefunden wurde. Das Problem ist erfüllbar.
	\item Zeit abgelaufen: Der Beweiser konnte sich in der zugewiesenen Zeit weder für 1. noch 2. entscheiden.
	\item Ungültig: Das Problem liegt in einer Syntax vor, die der Beweiser nicht verarbeiten kann. Die Beweissuche wird nicht ausgeführt.
\end{enumerate}
Das Ergebnis des gesamten Jobs ist eine ZIP-Datei, die für jede Paarung eine Ergebnisdatei mit dem Resultat enthält. Probleme, die gelöst wurden, enthalten in der Ergebnisdatei zusätzliche Informationen wie Laufzeit und Anzahl Resolutionsschritte.


	\section{PyRes}
PyRes ist ein Theorembeweiser, der in der Programmiersprache Python entwickelt wurde. Der Beweiser arbeitet mit Resolution in der Prädikatenlogik erster Stufe und ist somit vollständig. Der Fokus bei PyRes liegt nicht auf dem schnellstmöglichen Lösen von Problemen. Ziel ist, die Konzepte des Theorembeweisens anschaulich darzustellen und für Anfänger leicht verständlich zu machen. \cite{Schulz2020PyRes}

Das Programm kann beispielhaft folgendermaßen aufgerufen werden:

./pyres-fof.py -t -HPickGiven5 file.p

Die Beweissuche wird dann in mehreren Schritten durchgeführt. 
\begin{enumerate}
	\item Die Eingabeoptionen werden eingelesen und in einem Container-Objekt gespeichert.
	\item Die Datei file.p wird eingelesen und geparst. Die Datei kann sowohl Formeln (FOF-Format) als auch Klauseln (CNF-Format) enthalten.
	\item Alle Formeln werden in Klauseln umgewandelt. Sollte das Problem Formeln vom Typ "conjecture" (Vermutung) enthalten, so werden diese negiert.
	\item Die Beweissuche beginnt. Alle Klauseln sind in einer Klauselmenge $S_1$ namens unprocessed (unverarbeitet) gesammelt. Eine zweite Klauselmenge $S_2$ heißt processsed (verarbeitet) und ist anfangs leer. 
	\item Es wird eine Klausel $C_1$ aus $S_1$ ausgewählt und aus $S_1$ entfernt. Wenn $C_1$ die leeren Klausel ist, wurde ein Beweis gefunden. Die Suche ist abgeschlossen. Ist $C_1$ nicht leer, werden weitere Schritte unternommen:
	\begin{enumerate}
		\item Aus $C_1$ werden neuen Klauseln durch Faktorisierung erstellt.
		\item Aus $C_1$ werden in Kombination mit jeder Klausel aus $S_2$ neue Klauseln durch Resolution erstellt.
	\end{enumerate}
	Sowohl die Faktoren als auch die Resolventen werden zu $S_1$ hinzugefügt. Danach wird $C_1$ zu $S_2$ hinzugefügt. Dieser Schritt wird solange wiederholt, bis die leere Klausel gefunden wurde.
	\item Wenn die Suche abgeschlossen ist, wird eine Zusammenfassung des Beweises ausgegeben.
\end{enumerate}
PyRes ermöglicht diverse Optimierungen, die die Wahrscheinlichkeiten erhöhen, einen Beweis zu finden.
Der Anwender kann diese Optimierungen mit den Eingabeparametern angeben. Der Parameter "`t"' sorgt dafür, dass das Programm in jedem Beweisschritt prüft, ob die Klausel eine Tautologie ist, und sie gegebenenfalls löscht. Mit "`f"' und "`b"' können Vorwärts- und Rückwärtssubsumption aktiviert werden.
Der Benutzer kann mit der Option "`H"' eine Stategie auswählen, mit der PyRes die Klauseln aus der unverarbeiteten Menge $S_1$ auswählt. Mögliche Optionen sind FIFO, SymbolCount, PickGiven2 und PickGiven5. FIFO steht für "`first in first out"' und wählt in jedem Schritt diejenige Klausel aus, die am längsten in der Menge ist. SymbolCount wählt die Kausel mit den wenigsten Literalen aus, und PickGiven2 und PickGiven5 wechseln zwischen FIFO und SymbolCount in vorgegebenen Verhältnissen ab. Eine weitere Optimierung ist die Literalselektion, die die Anzahl der möglichen Kombinationen von Klauseln mindert.